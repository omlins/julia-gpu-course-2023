{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3 - **Data transfer optimisations (advanced, 2nd part)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to:\n",
    "- learn how to control registers for storing intermediate results on-chip;\n",
    "- learn how to use shared memory (on-chip) to communicate between threads within a block.\n",
    "- learn how to use warp-level functions to communicate between threads within a warp.\n",
    "\n",
    "Prerequisites:\n",
    "- the introduction notebook *Benchmarking memory copy and establishing peak memory access performance* (`1_memorycopy.ipynb`)\n",
    "- the first *Data transfer optimisation notebook* (`2_datatransfer_optimisations.ipynb`)\n",
    "- the second *Data transfer optimisation notebook* (`3_datatransfer_optimisations_advanced_part1.ipynb`)\n",
    "\n",
    "[*This content is distributed under MIT licence. Authors: S. Omlin (CSCS), L. RÃ¤ss (ETHZ).*](https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/LICENSE.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the packages `CUDA`, `BenchmarkTools` and `Plots` to create a little performance laboratory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/tmpwdir/julia-gpu-course-2023/solutions`\n"
     ]
    }
   ],
   "source": [
    "] activate ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "] instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using IJulia\n",
    "using BenchmarkTools\n",
    "using Plots\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last notebook (`3_datatransfer_optimisations.ipynb`), you learned how to explicitly control part of the the on-chip memory usage, using so called \"shared memory\". We will learn now how to control a second kind of fast memory on-chip: registers. To this purpose we will implement the `cumsum!` function on GPU - for the sake of simplicity, we will only write it for 3-D arrays.\n",
    "\n",
    "Here is the documentation of the function `cumsum!`\n",
    "```julia\n",
    "help?> CUDA.cumsum!\n",
    "  cumsum!(B, A; dims::Integer)\n",
    "\n",
    "  Cumulative sum of A along the dimension dims, storing the result in B. See\n",
    "  also cumsum.\n",
    "```\n",
    "\n",
    "And here is a small example of how cumsum! works for 3-D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4Ã—4Ã—4 CuArray{Float32, 3, CUDA.Mem.DeviceBuffer}:\n",
       "[:, :, 1] =\n",
       " 1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0\n",
       "\n",
       "[:, :, 3] =\n",
       " 3.0  3.0  3.0  3.0\n",
       " 3.0  3.0  3.0  3.0\n",
       " 3.0  3.0  3.0  3.0\n",
       " 3.0  3.0  3.0  3.0\n",
       "\n",
       "[:, :, 4] =\n",
       " 4.0  4.0  4.0  4.0\n",
       " 4.0  4.0  4.0  4.0\n",
       " 4.0  4.0  4.0  4.0\n",
       " 4.0  4.0  4.0  4.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.ones(4,4,4)\n",
    "B = CUDA.zeros(4,4,4)\n",
    "cumsum!(B, A; dims=1)\n",
    "cumsum!(B, A; dims=2)\n",
    "cumsum!(B, A; dims=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For benchmarking activities, we will allocate again large arrays, matching closely the number of grid points of the array size found best in the introduction notebook (you can modify the value if it is not right for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = ny = nz = 512\n",
    "A = CUDA.rand(Float64, nx, ny, nz);\n",
    "B = CUDA.zeros(Float64, nx, ny, nz);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we preallocate also an array to store reference results obtained from `CUDA.cumsum!` for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_ref = CUDA.zeros(Float64, nx, ny, nz);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are set up to get started.\n",
    "\n",
    "If we only consider main memory access, then the `cumsum!` will ideally need to read one array (`A`) and write one array (`B`). No other main memory access should be required. Let us therefore create an adhoc adaption of the effective memory throughput metric for iterative PDE solvers (presented in the second notebook) to this problem. We will call the metric `T_eff_cs` and compute it as follows:\n",
    "`T_eff_cs = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it`\n",
    "\n",
    "The upper bound of `T_eff_cs` is given by the maximum throughput achievable when copying exactly one aray to another. For this specific case, you might have measured in our benchmarks in the introduction notebook a slightly less good performance (measured 550 GB/s with the Tesla P100 GPU) then what we established as `T_peak`. Thus, we will consider here this measured value to be the upper bound of `T_eff_cs` and will refer to it as `T_peak_cs`.\n",
    "\n",
    "We will now adapt the corresponding 2-D memory copy kernel in the most straighforward way to work for 3-D arrays. Then, we will modify this memory copy code in little steps until we have a well performing `cumsum!` GPU function.\n",
    "\n",
    "Here is the adapted memory copy kernel from the introduction notebook and the `T_tot` we achieve with it (for the first two dimensions, we use again the thread/block configuration that we found best in the introduction notebook and we use one thread in the third dimension; you can modify the values if it is not right for you):\n",
    "> ðŸ’¡ Note that the usage of the variables `A` and `B` is reversed in comparison with the previous notebooks in order to match the documentation of cumsum!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "538.3342390452848"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function memcopy!(B, A)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    iz = (blockIdx().z-1) * blockDim().z + threadIdx().z\n",
    "    @inbounds B[ix,iy,iz] = A[ix,iy,iz]\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "threads = (32, 8, 1)\n",
    "blocks  = nx.Ã·threads\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads memcopy!($B, $A); synchronize() end\n",
    "T_tot = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first implement the cummulative sum over the third dimension (index `iz`). The first step is to read in A (and write B) in a way that will later allow to easily do the required cumsums, which is an inherently serial operation. However, we want to try to avoid a serious degradation of the memory throughput when doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (grid-stride loops)\n",
    "\n",
    "Modify the above `memcopy!` kernel to read in A and write B in a serial manner with respect to the third dimension (index `iz`), i.e., with parallelization only over the first two dimensions (index `ix` and `iy`). Launch the kernel with the same thread configuration as above and adapt the the block configuration for the third dimension correctly. Verify the correctness of your kernel. Then, compute `T_tot` and explain the measured performance.\n",
    "> ðŸ’¡ Hint: you need to launch the kernel with only one thread and one block in the third dimension and, inside the kernel, do a loop over the third dimension. Use `iz` as loop index as it will replace the previous `iz` computed from the thread location in the grid.\n",
    ">\n",
    "> ðŸ’¡ Note: The operator `â‰ˆ` allows to check if two arrays contain the same values (within a tolerance). Use this to verify your memory copy kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 64, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "function memcopy!(B, A)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    for iz = 1:size(A,3)\n",
    "        @inbounds B[ix,iy,iz] = A[ix,iy,iz]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "threads = (32, 8, 1)\n",
    "blocks  = (nxÃ·threads[1], nyÃ·threads[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification\n",
    "B .= 0.0;\n",
    "@cuda blocks=blocks threads=threads memcopy!(B, A); synchronize()\n",
    "B â‰ˆ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515.1406069901815"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads memcopy!($B, $A); synchronize() end\n",
    "T_tot = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You just implemented a so called [grid-stride loop](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/). It exhibits a good memory access pattern and it allows to easily reuse, e.g., intermediate results from previous iterations in the loop. You probably observe a `T_tot` that is a bit below the one measured in the previous experiment (measured 520 GB/s with the Tesla P100 GPU). There is certainly room to improve this memory copy kernel a bit, but we will consider it sufficient in order to continue with this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (controlling registers)\n",
    "\n",
    "Write a kernel `cumsum_dim3!` which computes the cumulative sum over the 3rd dimension of a 3-D array. To this purpose modify the memory copy kernel from Task 1. Verify the correctness of your kernel against `CUDA.cumsum!`. Then, compute `T_eff_cs` as defined above, explain the measured performance and compare it to the one obtained with the generic `CUDA.cumsum!`.\n",
    "> ðŸ’¡ Hint: define a variable `cumsum_iz` before the loop and initialize it to 0.0 in order to cumulate the sum.\n",
    ">\n",
    "> ðŸ’¡ Note: The operator `â‰ˆ` allows to check if two arrays contain the same values (within a tolerance). Use this to verify your results against `CUDA.cumsum!` (remember that for the verification, we already preallocated an array `B_ref` at the beginning, which you can use now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "function cumsum_dim3!(B, A)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    cumsum_iz = 0.0\n",
    "    for iz = 1:size(A,3)\n",
    "        @inbounds cumsum_iz  += A[ix,iy,iz]\n",
    "        @inbounds B[ix,iy,iz] = cumsum_iz\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# Verification\n",
    "@cuda blocks=blocks threads=threads cumsum_dim3!(B, A); synchronize()\n",
    "CUDA.cumsum!(B_ref, A; dims=3);\n",
    "B â‰ˆ B_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516.0146325803668"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads cumsum_dim3!($B, $A); synchronize() end\n",
    "T_eff_cs = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.7608589663653"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_it = @belapsed begin CUDA.cumsum!($B, $A; dims=3); synchronize() end\n",
    "T_eff_cs = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe no significant difference between `T_eff_cs` measured here and `T_tot` computed in Task 1 (measured 520 GB/s with the Tesla P100 GPU). In fact, scalar variables defined in a kernel, like `cumsum_iz`, will be stored in registers as long as there are enough available (if not, then so called [register spilling](https://developer.download.nvidia.com/CUDA/training/register_spilling.pdf) to main memory takes place). As access to register is extremely fast, the summation added in this Task did certainly not reduce the measured performance. It is, once again, the memory copy speed that completely determines the performance of the kernel, because we have successfully controlled the use of registers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the cummulative sum over the 2nd dimension (index `iy`). As for the previous implementation, the first step is to read in A (and write B) in a way that will later allow to easily do the required cumsums without exhibiting significant memory throughput degradation.\n",
    "### Task 3 (kernel loops)\n",
    "\n",
    "Modify the `memcopy!` kernel given in the beginning to read in A and write B in a serial manner with respect to the second dimension (index `iy`), i.e., with parallelization only over the first and the last dimensions (index `ix` and `iz`). Launch the kernel with the same amount of threads as in Task 1, however, place them all in the first dimension (i.e. use one thread in the second and third dimensions); adapt the the block configuration correctly. Verify the correctness of your kernel. Then, compute `T_tot` and explain the measured performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "function memcopy!(B, A)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iz = (blockIdx().z-1) * blockDim().z + threadIdx().z\n",
    "    for iy = 1:size(A,2)\n",
    "        @inbounds B[ix,iy,iz] = A[ix,iy,iz]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "threads = (256, 1, 1)\n",
    "blocks  = (nxÃ·threads[1], 1, nzÃ·threads[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification\n",
    "B .= 0.0;\n",
    "@cuda blocks=blocks threads=threads memcopy!(B, A); synchronize()\n",
    "B â‰ˆ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511.4990258221442"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads memcopy!($B, $A); synchronize() end\n",
    "T_tot = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe no big difference between `T_tot` measured here and `T_tot` computed in Task 1 (measured 519 GB/s with the Tesla P100 GPU) as this kernel is accessing memory also in large contiguous chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (controlling registers)\n",
    "\n",
    "Write a kernel `cumsum_dim2!` which computes the cumulative sum over the 2nd dimension of a 3-D array. To this purpose modify the memory copy kernel from Task 3. Verify the correctness of your kernel against `CUDA.cumsum!`. Then, compute `T_eff_cs` as defined above, explain the measured performance and compare it to the one obtained with the generic `CUDA.cumsum!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "function cumsum_dim2!(B, A)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iz = (blockIdx().z-1) * blockDim().z + threadIdx().z\n",
    "    cumsum_iy = 0.0\n",
    "    for iy = 1:size(A,2)\n",
    "        @inbounds cumsum_iy  += A[ix,iy,iz]\n",
    "        @inbounds B[ix,iy,iz] = cumsum_iy\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# Verification\n",
    "@cuda blocks=blocks threads=threads cumsum_dim2!(B, A); synchronize()\n",
    "CUDA.cumsum!(B_ref, A; dims=2);\n",
    "B â‰ˆ B_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510.2616594191931"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads cumsum_dim2!($B, $A); synchronize() end\n",
    "T_eff_cs = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.83477386471998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_it = @belapsed begin CUDA.cumsum!($B, $A; dims=2); synchronize() end\n",
    "T_eff_cs = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you should observe no significant difference between `T_eff_cs` measured here and `T_tot` computed in Task 3 (measured 519 GB/s with the Tesla P100 GPU), as you probably expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the cummulative sum over the 1st dimension (index `ix`). As for the previous implementations, the first step is to read in A (and write B) in a way that will later allow to easily do the required cumsums without exhibiting significant memory throughput degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (kernel loops)\n",
    "\n",
    "Modify the `memcopy!` kernel given in the beginning to read in A and write B in a serial manner with respect to the first dimension (index `ix`), i.e., with parallelization only over the second and third dimensions (index `iy` and `iz`). Launch the kernel with the same amount of threads as in Task 1, however, place them all in the second dimension (i.e. use one thread in the first and third dimensions); adapt the the block configuration correctly. Verify the correctness of your kernel. Then, think about what performance you expect, compute `T_tot` and explain the measured performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 512)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "function memcopy!(B, A)\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    iz = (blockIdx().z-1) * blockDim().z + threadIdx().z\n",
    "    for ix = 1:size(A,1)\n",
    "        @inbounds B[ix,iy,iz] = A[ix,iy,iz]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "threads = (1, 256, 1)\n",
    "blocks  = (1, nyÃ·threads[2], nzÃ·threads[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification\n",
    "B .= 0.0;\n",
    "@cuda blocks=blocks threads=threads memcopy!(B, A); synchronize()\n",
    "B â‰ˆ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.6334300070898"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads memcopy!($B, $A); synchronize() end\n",
    "T_tot = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You likely observe `T_tot` to be an order of magnitude or more below `T_tot` measured in Task 1 and 3 (measured 36 GB/s with the Tesla P100 GPU) because, in contrast to the previous kernels, this kernel acceses memory discontiguously with a large stride (of `nx` numbers) between each requested number.\n",
    "\n",
    "There obviously is no point in creating a `cumsum!` kernel out of this `memcopy!` kernel: the performance could only be the same or worse (even though worse is not to expect). Hence, we will try to improve this memory copy kernel by accessing multiple contigous numbers from memory, which we can achieve by launching more then one threads in the first dimension, adapting the loop accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (kernel loops)\n",
    "\n",
    "Modify the `memcopy!` kernel from Task 5 to enable reading in 32 numbers at a time in the first dimension (index `ix`) rather than one number at a time as before. Launch the kernel with just 32 threads, all placed in the first dimension; adapt the the block configuration if you need to. Verify the correctness of your kernel. Then, think about what performance you expect now, compute `T_tot` and explain the measured performance.\n",
    "> ðŸ’¡ Hint: you could hardcode the kernel to read 32 numbers at a time, but we prefere to write it more generally allowing to launch the kernel with a different number of threads in the first dimension (however, we do not want to enable more then one block though in this dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 512)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "function memcopy!(B, A)\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    iz = (blockIdx().z-1) * blockDim().z + threadIdx().z\n",
    "    for ix_offset = 0 : blockDim().x : size(A,1)-1\n",
    "        ix = threadIdx().x + ix_offset\n",
    "        @inbounds B[ix,iy,iz] = A[ix,iy,iz]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "threads = (32, 1, 1)\n",
    "blocks  = (1, nyÃ·threads[2], nzÃ·threads[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification\n",
    "B .= 0.0;\n",
    "@cuda blocks=blocks threads=threads memcopy!(B, A); synchronize()\n",
    "B â‰ˆ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525.3262118371147"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads memcopy!($B, $A); synchronize() end\n",
    "T_tot = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`T_tot` should now be similar to the one obtained in Task 1 and 3 or even a bit better (measured 534 GB/s with the Tesla P100 GPU) thanks to the additional concurrency compared to the other `memcopy!` versions. We are therefore ready to implement the cummulative sum over the 1st dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (communication via shared memory)\n",
    "\n",
    "Write a kernel `cumsum_dim1!` which computes the cumulative sum over the 1st dimension of a 3-D array. To this purpose modify the memory copy kernel from Task 6. Verify the correctness of your kernel against `CUDA.cumsum!`. Then, compute `T_eff_cs` as defined above, explain the measured performance.\n",
    "> ðŸ’¡ Hint: if you read the data into shared memory, then you can compute the cumulative sum, e.g., with the first thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cumsum_dim1! (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "function cumsum_dim1!(B, A)\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    iz = (blockIdx().z-1) * blockDim().z + threadIdx().z\n",
    "    tx = threadIdx().x\n",
    "    shmem = CuDynamicSharedArray(eltype(A), blockDim().x)\n",
    "    cumsum_ix = 0.0\n",
    "    for ix_offset = 0 : blockDim().x : size(A,1)-1\n",
    "        ix = threadIdx().x + ix_offset\n",
    "        @inbounds shmem[tx] = A[ix,iy,iz]       # Read the x-dimension chunk into shared memory.\n",
    "        sync_threads()\n",
    "        if tx == 1                              # Compute the cumsum only with the first thread, accessing only shared memory\n",
    "            for i = 1:blockDim().x\n",
    "                @inbounds cumsum_ix += shmem[i]\n",
    "                @inbounds shmem[i] = cumsum_ix\n",
    "            end\n",
    "        end\n",
    "        sync_threads()\n",
    "        @inbounds B[ix,iy,iz] = shmem[tx]       # Write the x-dimension chunk, now containing results, to main memory.\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification\n",
    "@cuda blocks=blocks threads=threads shmem=prod(threads)*sizeof(Float64) cumsum_dim1!(B, A); synchronize()\n",
    "CUDA.cumsum!(B_ref, A; dims=1);\n",
    "B â‰ˆ B_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335.56204187033626"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads shmem=2*prod($threads)*sizeof(Float64) cumsum_dim1!($B, $A); synchronize() end\n",
    "T_eff_cs = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`T_eff_cs` (measured 336 GB/s with the Tesla P100 GPU) is probably significantly less good than the one obtained for the cumulative sums over the other dimensions, but still quite good if we keep in mind the `T_tot` achieved with the first memcopy manner in Task 5. A good strategy for tackling an optimal implementation is certainly to use warp-level functions to communicate between the threads and if needed a more complex summation algorithm. Let us try with warp-level functions, using a similarly simple summation algorithm as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (communication with warp-level functions)\n",
    "\n",
    "Rewrite the kernel `cumsum_dim1!` which computes the cumulative sum over the 1st dimension of a 3-D array: communicate between the threads using warp-level functions instead of shared memory as done in Task 7. To this purpose modify the memory copy kernel from Task 6. Verify the correctness of your kernel against `CUDA.cumsum!`. Then, compute `T_eff_cs` as defined above, explain the measured performance.\n",
    "> ðŸ’¡ Hint: read with each thread one value of the array `A` and use [shfl_sync](https://cuda.juliagpu.org/stable/api/kernel/#CUDA.shfl_sync) to communicate the value to all other threads. Then, each thread can compute the cumulative sum (redundantly); write with each thread its corresponding cumulative sum to array `B`.\n",
    ">\n",
    "> ðŸ’¡ Hint: you could hardcode the kernel to read 32 numbers at a time, but we prefere to write it more generally allowing to launch the kernel with a smaller number of threads in the first dimension (however, we do not want to enable more than one block in this dimension, nor more than one warp per block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cumsum_dim1! (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "function cumsum_dim1!(B, A)\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    iz = (blockIdx().z-1) * blockDim().z + threadIdx().z\n",
    "    tx = threadIdx().x\n",
    "    cumsum_ix = 0.0\n",
    "    for ix_offset = 0 : blockDim().x : size(A,1)-1\n",
    "        ix = threadIdx().x + ix_offset\n",
    "        @inbounds val = A[ix,iy,iz]               # Read the x-dimension chunk into thread-local registers.\n",
    "        cumsum_tx = 0.0\n",
    "        for i = 1:blockDim().x\n",
    "            val_i = shfl_sync(CUDA.FULL_MASK, val, i) # Communicate each threads value to all the other threads.\n",
    "            cumsum_ix += val_i\n",
    "            if i == tx\n",
    "                cumsum_tx = cumsum_ix\n",
    "            end\n",
    "        end\n",
    "        @inbounds B[ix,iy,iz] = cumsum_tx         # Write the x-dimension chunk of results to main memory.\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification\n",
    "@cuda blocks=blocks threads=threads cumsum_dim1!(B, A); synchronize()\n",
    "CUDA.cumsum!(B_ref, A; dims=1);\n",
    "B â‰ˆ B_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163.2021181858322"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads cumsum_dim1!($B, $A); synchronize() end\n",
    "T_eff_cs = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`T_eff_cs` (measured 163 GB/s with the Tesla P100 GPU) might be disappointing to you and less good than the one with communication between threads using shared memory. To tackle an optimal implementation we need to reduce the amount of communication between the threads. To this aim, we need to use a smarter, partly parallel summation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9 (communication with warp-level functions)\n",
    "\n",
    "Rewrite the kernel `cumsum_dim1!` which computes the cumulative sum over the 1st dimension of a 3-D array: reduce the amount of communication between the threads compared to Task 8 by using a smarter, partly parallel summation algorithm. To this purpose modify the kernel from Task 8. Verify the correctness of your kernel against `CUDA.cumsum!`. Then, compute `T_eff_cs` as defined above, explain the measured performance and compare it to the one obtained with the generic `CUDA.cumsum!`.\n",
    "> ðŸ’¡ Hint: a partly parallel algorithm to compute the cumsum of an x-dimension chunk is to 1) compute the local cumsum for subchunks of increasing size, `1, 2, 4, 8, 16, 32`, where:\n",
    "        the local cumsum of the subchunks of size `1` is simply `A[ix,iy,iz]`, and\n",
    "        the local cumsum of the subchunks of size `>1` is the previous value plus the one of the lower half (at position `subchunk size / 2`).\n",
    "    Then, 2) compute the global cumsum of the thread by adding the global result of the previous chunk.\n",
    "    Finally, 3) broadcast the global cumsum of the last thread to all other threads as they will require it to compute the next chunk.\n",
    ">\n",
    "> ðŸ’¡ Hint: use [shfl_sync](https://cuda.juliagpu.org/stable/api/kernel/#CUDA.shfl_sync) with the optional argument `width` to communicate efficiently within subchunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cumsum_dim1! (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "function cumsum_dim1!(B, A)\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    iz = (blockIdx().z-1) * blockDim().z + threadIdx().z\n",
    "    tx = threadIdx().x\n",
    "    pow2 = Int(log2(blockDim().x)) \n",
    "    cumsum_ix = 0.0\n",
    "    for ix_offset = 0 : blockDim().x : size(A,1)-1\n",
    "        ix = threadIdx().x + ix_offset\n",
    "        @inbounds val = A[ix,iy,iz]               # Read the x-dimension chunk into thread-local registers.\n",
    "        cumsum_tx = val\n",
    "        pos   = 1\n",
    "        width = 2\n",
    "        for i = 1:pow2\n",
    "            cumsum_lower = shfl_sync(CUDA.FULL_MASK, cumsum_tx, pos, width)\n",
    "            if ((tx-1) % width) > pos-1\n",
    "                cumsum_tx += cumsum_lower\n",
    "            end\n",
    "            pos   *= 2                            # pos   = 2^(pow2-1)\n",
    "            width *= 2                            # width = 2^pow2\n",
    "        end\n",
    "        cumsum_tx += cumsum_ix\n",
    "        @inbounds B[ix,iy,iz] = cumsum_tx         # Write the x-dimension chunk of results to main memory.\n",
    "        cumsum_ix = shfl_sync(CUDA.FULL_MASK, cumsum_tx, blockDim().x)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verification\n",
    "@cuda blocks=blocks threads=threads cumsum_dim1!(B, A); synchronize()\n",
    "CUDA.cumsum!(B_ref, A; dims=1);\n",
    "B â‰ˆ B_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523.0874275131711"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads cumsum_dim1!($B, $A); synchronize() end\n",
    "T_eff_cs = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108.75452198162124"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_it = @belapsed begin CUDA.cumsum!($B, $A; dims=1); synchronize() end\n",
    "T_eff_cs_generic = 2*1/1e9*nx*ny*nz*sizeof(Float64)/t_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented cumulative summation in an optimized fashion using warp-level functions! `T_eff_cs` (measured 524 GB/s with the Tesla P100 GPU) should now be equal or almost equal to the memory copy throughput measured in Task 6, which was our starting point for this kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10 (Performance evaluation)\n",
    "\n",
    "Compute by how much percent you can further improve the performance of `cumsum_dim1!` at most. To this purpose, remember the following from the introduction of this notebook: \n",
    "> The upper bound of `T_eff_cs` is given by the maximum throughput achievable when copying exactly one aray to another. For this specific case, you might have measured in our benchmarks in the introduction notebook a slightly less good performance (measured 550 GB/s with the Tesla P100 GPU) then what we established as `T_peak`. Thus, we will consider here this measured value to be the upper bound of `T_eff_cs` and will refer to it as `T_peak_cs`.\n",
    "\n",
    "Furthermore, reflect on which strategy you could use to further optimize `cumsum_dim1!` and which additional challenges you would have to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9510680500239473"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "T_peak_cs = 550 # Peak memory throughput of the Tesla P100 GPU to copy exactly one array to another\n",
    "T_eff_cs/T_peak_cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from a memory copy kernel with a throughput possibly more than an order of magnitude below `T_peak_cs`, we have implemented cumulative summation with a performance close to `T_peak_cs`, the theoretical upper bound for this computation. This completes the third and last part on data transfer optimisations with Julia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0-rc1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
